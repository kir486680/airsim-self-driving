{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finding Lane Lines on the Road** \n",
    "Built by Galen Ballew in December 2016 through the Udacity Self Driving Car Engineer Nanodegree*\n",
    "***\n",
    "In this project, I built a computer vision pipeline for detecting lane lines and creating averaged and extrapolated boundary lines.\n",
    "The pipeline is as follows: \n",
    "\n",
    "1) Convert frame to grayscale  \n",
    "2) Create masks for yellow and white pixels  \n",
    "3) Apply a Gaussian smoothing  \n",
    "4) Apply a Canny edge detection  \n",
    "5) Create an additional mask to focus on the \"region of interest\" in front of the vehicle  \n",
    "6) Convert the points(i.e. pixels) in XY space to a line in Hough space  \n",
    "7) Where the lines in Hough space intersect (i.e. a point) a line exists in XY space  \n",
    "8) Using the extrema of the lines generated, create two averaged line  s\n",
    "9) Create two averaged lines across frames for a smooth video playback  \n",
    "10) Draw the lines to each frame\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    " <img src=\"line-segments-example.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Raw lines after conversion from Hough space </p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n",
    "<figure>\n",
    " <img src=\"laneLines_thirdPass.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Averaged lines for smooth playback</p> \n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "#importing some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale(img):\n",
    "    \"\"\"Applies the Grayscale transform\n",
    "    This will return an image with only one color channel\n",
    "    but NOTE: to see the returned image as grayscale\n",
    "    (assuming your grayscaled image is called 'gray')\n",
    "    you should call plt.imshow(gray, cmap='gray')\"\"\"\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Or use BGR2GRAY if you read an image with cv2.imread()\n",
    "    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "def canny(img, low_threshold, high_threshold):\n",
    "    \"\"\"Applies the Canny transform\"\"\"\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "#used below\n",
    "def get_slope(x1,y1,x2,y2):\n",
    "    return (y2-y1)/(x2-x1)\n",
    "\n",
    "#thick red lines \n",
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=6):\n",
    "    \"\"\"workflow:\n",
    "    1) examine each individual line returned by hough & determine if it's in left or right lane by its slope\n",
    "    because we are working \"upside down\" with the array, the left lane will have a negative slope and right positive\n",
    "    2) track extrema\n",
    "    3) compute averages\n",
    "    4) solve for b intercept \n",
    "    5) use extrema to solve for points\n",
    "    6) smooth frames and cache\n",
    "    \"\"\"\n",
    "    global cache\n",
    "    global first_frame\n",
    "    y_global_min = img.shape[0] #min will be the \"highest\" y value, or point down the road away from car\n",
    "    y_max = img.shape[0]\n",
    "    l_slope, r_slope = [],[]\n",
    "    l_lane,r_lane = [],[]\n",
    "    det_slope = 0.4\n",
    "    α =0.2 \n",
    "    #i got this alpha value off of the forums for the weighting between frames.\n",
    "    #i understand what it does, but i dont understand where it comes from\n",
    "    #much like some of the parameters in the hough function\n",
    "    \n",
    "    for line in lines:\n",
    "        #1\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            slope = get_slope(x1,y1,x2,y2)\n",
    "            if slope > det_slope:\n",
    "                r_slope.append(slope)\n",
    "                r_lane.append(line)\n",
    "            elif slope < -det_slope:\n",
    "                l_slope.append(slope)\n",
    "                l_lane.append(line)\n",
    "        #2\n",
    "        y_global_min = min(y1,y2,y_global_min)\n",
    "    \n",
    "    # to prevent errors in challenge video from dividing by zero\n",
    "    if((len(l_lane) == 0) or (len(r_lane) == 0)):\n",
    "        print ('no lane detected')\n",
    "        return 1\n",
    "        \n",
    "    #3\n",
    "    l_slope_mean = np.mean(l_slope,axis =0)\n",
    "    r_slope_mean = np.mean(r_slope,axis =0)\n",
    "    l_mean = np.mean(np.array(l_lane),axis=0)\n",
    "    r_mean = np.mean(np.array(r_lane),axis=0)\n",
    "    \n",
    "    if ((r_slope_mean == 0) or (l_slope_mean == 0 )):\n",
    "        print('dividing by zero')\n",
    "        return 1\n",
    "    \n",
    "   \n",
    "    \n",
    "    #4, y=mx+b -> b = y -mx\n",
    "    l_b = l_mean[0][1] - (l_slope_mean * l_mean[0][0])\n",
    "    r_b = r_mean[0][1] - (r_slope_mean * r_mean[0][0])\n",
    "    \n",
    "    #5, using y-extrema (#2), b intercept (#4), and slope (#3) solve for x using y=mx+b\n",
    "    # x = (y-b)/m\n",
    "    # these 4 points are our two lines that we will pass to the draw function\n",
    "    l_x1 = int((y_global_min - l_b)/l_slope_mean) \n",
    "    l_x2 = int((y_max - l_b)/l_slope_mean)   \n",
    "    r_x1 = int((y_global_min - r_b)/r_slope_mean)\n",
    "    r_x2 = int((y_max - r_b)/r_slope_mean)\n",
    "    \n",
    "    #6\n",
    "    if l_x1 > r_x1:\n",
    "        l_x1 = int((l_x1+r_x1)/2)\n",
    "        r_x1 = l_x1\n",
    "        l_y1 = int((l_slope_mean * l_x1 ) + l_b)\n",
    "        r_y1 = int((r_slope_mean * r_x1 ) + r_b)\n",
    "        l_y2 = int((l_slope_mean * l_x2 ) + l_b)\n",
    "        r_y2 = int((r_slope_mean * r_x2 ) + r_b)\n",
    "    else:\n",
    "        l_y1 = y_global_min\n",
    "        l_y2 = y_max\n",
    "        r_y1 = y_global_min\n",
    "        r_y2 = y_max\n",
    "      \n",
    "    current_frame = np.array([l_x1,l_y1,l_x2,l_y2,r_x1,r_y1,r_x2,r_y2],dtype =\"float32\")\n",
    "    \n",
    "    if first_frame == 1:\n",
    "        next_frame = current_frame        \n",
    "        first_frame = 0        \n",
    "    else :\n",
    "        prev_frame = cache\n",
    "        next_frame = (1-α)*prev_frame+α*current_frame\n",
    "             \n",
    "    cv2.line(img, (int(next_frame[0]), int(next_frame[1])), (int(next_frame[2]),int(next_frame[3])), color, thickness)\n",
    "    cv2.line(img, (int(next_frame[4]), int(next_frame[5])), (int(next_frame[6]),int(next_frame[7])), color, thickness)\n",
    "    \n",
    "    cache = next_frame\n",
    "    \n",
    "\n",
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    \"\"\"\n",
    "    `img` should be the output of a Canny transform.\n",
    "        \n",
    "    Returns an image with hough lines drawn.\n",
    "    \"\"\"\n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "    draw_lines(line_img,lines)\n",
    "    return line_img\n",
    "\n",
    "# Python 3 has support for cool math symbols.\n",
    "\n",
    "def weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n",
    "    \"\"\"\n",
    "    `img` is the output of the hough_lines(), An image with lines drawn on it.\n",
    "    Should be a blank image (all black) with lines drawn on it.\n",
    "    \n",
    "    `initial_img` should be the image before any processing.\n",
    "    \n",
    "    The result image is computed as follows:\n",
    "    \n",
    "    initial_img * α + img * β + λ\n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, α, img, β, λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Images\n",
    "\n",
    "Run on single still frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1080, 3)\n",
      "no lane detected\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_image(image):\n",
    "    \n",
    "    global first_frame\n",
    "\n",
    "    gray_image = grayscale(image)\n",
    "    img_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    #hsv = [hue, saturation, value]\n",
    "    #more accurate range for yellow since it is not strictly black, white, r, g, or b\n",
    "\n",
    "    lower_yellow = np.array([20, 100, 100], dtype = \"uint8\")\n",
    "    upper_yellow = np.array([30, 255, 255], dtype=\"uint8\")\n",
    "\n",
    "    mask_yellow = cv2.inRange(img_hsv, lower_yellow, upper_yellow)\n",
    "    mask_white = cv2.inRange(gray_image, 200, 255)\n",
    "    mask_yw = cv2.bitwise_or(mask_white, mask_yellow)\n",
    "    mask_yw_image = cv2.bitwise_and(gray_image, mask_yw)\n",
    "\n",
    "    kernel_size = 5\n",
    "    gauss_gray = gaussian_blur(mask_yw_image,kernel_size)\n",
    "\n",
    "    #same as quiz values\n",
    "    low_threshold = 50\n",
    "    high_threshold = 150\n",
    "    canny_edges = canny(gauss_gray,low_threshold,high_threshold)\n",
    "\n",
    "    imshape = image.shape\n",
    "    lower_left = [imshape[1]/9,imshape[0]]\n",
    "    lower_right = [imshape[1]-imshape[1]/9,imshape[0]]\n",
    "    top_left = [imshape[1]/2-imshape[1]/8,imshape[0]/2+imshape[0]/10]\n",
    "    top_right = [imshape[1]/2+imshape[1]/8,imshape[0]/2+imshape[0]/10]\n",
    "    vertices = [np.array([lower_left,top_left,top_right,lower_right],dtype=np.int32)]\n",
    "    roi_image = region_of_interest(canny_edges, vertices)\n",
    "\n",
    "    #rho and theta are the distance and angular resolution of the grid in Hough space\n",
    "    #same values as quiz\n",
    "    rho = 4\n",
    "    theta = np.pi/180\n",
    "    #threshold is minimum number of intersections in a grid for candidate line to go to output\n",
    "    threshold = 30\n",
    "    min_line_len = 100\n",
    "    max_line_gap = 180\n",
    "    #my hough values started closer to the values in the quiz, but got bumped up considerably for the challenge video\n",
    "\n",
    "    line_image = hough_lines(roi_image, rho, theta, threshold, min_line_len, max_line_gap)\n",
    "    result = weighted_img(line_image, image, α=0.8, β=1., λ=0.)\n",
    "    return result\n",
    "\n",
    "image = cv2.imread(\"pic.jpg\")\n",
    "print(image.shape)\n",
    "processed = process_image(image)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Videos\n",
    "\n",
    "Apply the still frame processing to video feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video white.mp4\n",
      "[MoviePy] Writing video white.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████▋| 221/222 [00:02<00:00, 86.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: white.mp4 \n",
      "\n",
      "Wall time: 2.81 s\n"
     ]
    }
   ],
   "source": [
    "first_frame = 1\n",
    "white_output = 'white.mp4'\n",
    "clip1 = VideoFileClip(\"solidWhiteRight.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_frame = 1\n",
    "yellow_output = 'yellow.mp4'\n",
    "clip2 = VideoFileClip('solidYellowLeft.mp4')\n",
    "yellow_clip = clip2.fl_image(process_image)\n",
    "%time yellow_clip.write_videofile(yellow_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(yellow_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflections\n",
    "\n",
    "\n",
    "1) create an ROI function that doesnt just use proportion of the frame to identify the polygon we are interested in. if you were driving a car of different height (think of how high up a semi cab is) or using a different camera, you'd always want the ROI to be accurate. what if the car is going up or down hill? what if there is another vehicle directly in front of the camera? what about rain, fog, and especially snow?! (these driving conditions will also have a great effect upon the color recognition, see 2) the best approach seems to be using lidar like sebastion thrun and the stanford team did for the great race. this is not as cost effective as a camera however.\n",
    "\n",
    "2) driving at nighttime. I don't have any footage of this, but different strength or colored headlights could affect the color thresholds, especially for the yellow lanes. any reflective lanes should be recognized as white, but that could be an issue. needs more testing. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Challenge\n",
    "\n",
    "This segment of video was a particular challenge due to the curve in the road. There were multiple intersections in Hough space that result in too many lines in XY. The averaged lines had to be modulated in order to be aware of the 'rolling extrema' of the other line (so that they don't cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_frame = 1\n",
    "challenge_output = 'extra.mp4'\n",
    "clip2 = VideoFileClip('challenge.mp4')\n",
    "challenge_clip = clip2.fl_image(process_image)\n",
    "%time challenge_clip.write_videofile(challenge_output, audio=False)\n",
    "#TODO how do we make curved ROI and curved lines? we need some calculus up in this bizzzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(challenge_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
